# 11주차-자동차 구매 의사결정

## 데이터셋

[car_data.csv](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/car_data.csv)

<aside>
💡 변수 설명

```
User ID : 고객 id
Gender : 성별
Age : 나이
Annual Salary : 연봉
Purchased : 자동차 구매여부
```

</aside>

## 사용한 라이브러리

```jsx
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
```

## 데이터 로드

```
car_df = pd.read_csv('/content/car_data.csv')
car_df
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-1.png)

```
car_df.info()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-2.png)

```
car_df.isnull().sum()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-3.png)

- 결측치 없음

## 성별에 따른 자동차 구매여부 시각화

```
sns.countplot(data=car_df, x='Gender', hue='Purchased')
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-4.png)

- 남성보다 여성이 자동차를 구매한 비율이 더 높음을 파악할 수 있음

## 성별에 따른 평균 연봉 시각화

```
car_mean = car_df.groupby('Gender')['AnnualSalary'].mean()
car_mean = pd.DataFrame(car_mean)
car_mean.plot.bar()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-5.png)

- 여성이 남성보다 평균 연봉이 높음을 알 수 있음

## 성별에 따른 자동차 구매여부 시각화

```
sns.countplot(data=car_df, x='Gender', hue='Purchased')
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-6.png)

- 평균 연봉이 높은 여성이 자동차 구매를 더 많이한다는 것을 파악함

## 연령에 따른 평균 연봉 파악 및 자동차 구매 여부 시각화

### 연령대 집단 범주화

```
car_df['age_group'] = ""
car_df.loc[(car_df['Age'] >= 10) & (car_df['Age'] < 20), 'age_group'] = '10대'
car_df.loc[(car_df['Age'] >= 20) & (car_df['Age'] < 30), 'age_group'] = '20대'
car_df.loc[(car_df['Age'] >= 30) & (car_df['Age'] < 40), 'age_group'] = '30대'
car_df.loc[(car_df['Age'] >= 40) & (car_df['Age'] < 50), 'age_group'] = '40대'
car_df.loc[(car_df['Age'] >= 50) & (car_df['Age'] < 60), 'age_group'] = '50대'
car_df.loc[(car_df['Age'] >= 60) & (car_df['Age'] < 70), 'age_group'] = '60대'
car_df
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-7.png)

- 연령에 따른 평균 연봉 및 자동차 구매여부 시각화를 위해 loc함수를 활용해 조건문을 걸어 10대~60대까지의 범주화를 수행
    - 여기서 새로 깨달은점!
        - loc함수에서 조건부를 활용할 시, 조건문 연산자를 사용하기 위해서는 ()괄호를 통해 조건을 묶어줘야 함!

### 연령대 집단별 평균 연봉

```
car_age = car_df.groupby('age_group')['AnnualSalary'].mean()

car_age = pd.DataFrame(car_age)
car_age
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-8.png)

```
#한글깨짐 방지
import matplotlib

matplotlib.rcParams['font.family'] ='Malgun Gothic'

matplotlib.rcParams['axes.unicode_minus'] =False

car_age.plot.bar()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-9.png)

- 30~60대 집단의 평균연봉이 대체적으로 높음을 파악할 수 있음

```
car_purchase = car_df.loc[car_df['Purchased'] == 1]
fig = px.histogram(car_purchase, x='age_group', y='Purchased', color='age_group')
fig.show()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-10.png)

- 자동차 구매 여부를 확인하는 것이기 때문에, car_df 데이터에서 Purchased변수 값이 1인 행만 추출하여 시각화 진행
- 평균 연봉이 높았던 40대, 50대 집단이 자동차 구매를 많이 했다는 것을 파악할 수 있음
- 평균 연봉이 높은 축에 속했던 30대와 60대는 평균 연봉에 비해 자동차 구매 횟수가 적음을 알 수 있음

### 연령과 자동차 구매의 상자그림 시각화

```
sns.boxplot(data=car_df, x='Purchased', y='Age')
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-11.png)

- 또 다르게 시각화를 진행해보자면, 상자그림을 활용하여 자동차 구매 여부와 관련한 연령대를 살펴볼 수 있음
    - 평균적으로 30~40살 사이에 해당하는 사람들은 자동차를 구매하지 않고, 40~50살 사이에 해당하는 사람들이 주로 자동차 구매를 한다는 것을 파악할 수 있음

## 연봉이 자동차 구매에 영향을 미칠까?

```
print(car_df['AnnualSalary'].min(), car_df['AnnualSalary'].max())
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-12.png)

- 연봉의 최솟값, 최대값 출력

```
labels = ['42500미만', '70000미만', '97500미만', '125000미만', '152500미만']
car_df['Salary_cut'] = pd.cut(car_df['AnnualSalary'], 5, labels = labels) 
car_df
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-13.png)

- bin값을 5로 지정하여 5개의 범주로 나누고, labels 리스트에 들어가있는 값들을 각 범주의 이름으로 부여
- if labels옵션을 지정하지 않으면?
    
    ```
    car_df['Salary_cut'] = pd.cut(car_df['AnnualSalary'], 5)
    car_df.Salary_cut.unique
    ```
    
    ![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-14.png)
    
    - 속성값이 위의 값들로 채워짐

```
car_salary_cut = car_df.loc[car_df['Purchased'] == 1]
fig = px.histogram(car_salary_cut, x='Salary_cut', y='Purchased', color='Salary_cut')
fig.show()
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-15.png)

- 연봉이 가장 작은 집단에서 가장 높은 자동차 구매율을 보였으며, 연봉이 자동차 구매에 큰 영향을 미치지 않을 것으로 판단됨

---

<aside>
💡 모델링 : SVM 사용

</aside>

## 모델링 전 데이터 전처리

```
X_num = car_df[['Age', 'AnnualSalary']]
X_cat = car_df['Gender']
car_dummies = pd.get_dummies(X_cat)
```

```
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_num)
X_scaled = pd.DataFrame(X_scaled, index = X_num.index, columns = X_num.columns)
X = pd.concat([X_scaled, car_dummies], axis=1)
Y = car_df['Purchased'] #타겟변수
```

```
X
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-16.png)

- 수치형 변수를 X_num에 담아 스케일링 전처리를 수행
- 범주형 변수인 Gender는 pd.get_dummies를 통해 수치형 변수로 바꿔 스케일링을 수행한 X_scaled 변수와 pd.concat 함수를 사용해 병합하고, 모델에 적용할 변수 X 생성

## 데이터 분할

캐글에서 test 데이터 셋을 제공하지 않았기 때문에 주어진 데이터 셋을 활용하여 데이터 셋을 학습용, 검증용 데이터로 분할할 필요가 있음

```
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=77)
print(X_train.shape, X_test.shape)
print(y_train.shape, y_test.shape)
```

## 모델 학습

```
import os
n_cpu = os.cpu_count()
n_thread = n_cpu*2
```

```
#필요한 라이브러리 탑재
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
```

```
model_svc = svm.SVC()

param = { 'C' : [0.5, 0.9, 1, 5, 10], 'kernel' : ['linear', 'rbf', 'poly'], 'gamma' : [0.1, 1, 10]}
GS_SVC = GridSearchCV(model_svc, param, cv=5, scoring='accuracy', n_jobs=n_thread)
GS_SVC.fit(X_train, y_train)
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-17.png)

- param 변수를 만들어 딕셔너리를 활용해 다양한 파라미터 값을 담음
- GridSearchCV를 활용하여 각 파라미터 값들을 모두 반영해 최적의 파라미터 값을 찾아냄
    - 5 X 3 X 3 번(각 파라미터의 개수)의 연산을 수행하여 속도가 조금 느림!

```
print('best_param : ', GS_SVC.best_params_)
print('best_score : ', GS_SVC.best_score_)
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-18.png)

- 최적의 파라미터 값과, 해당 파라미터를 적용했을 떄의 정확도를 출력

## 모델 예측

```
pred = GS_SVC.predict(X_test)
print(classification_report(y_test, pred))
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-19.png)

```
pred
```

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-20.png)

- 예측값이 들어있는 pred 변수 값을 출력해보면, 각 행에 대한 예측값들이 들어가져 있음
- 실제 존재하는 테스트 데이터 셋인 y_test와 예측한 pred값들을 비교하여 classification_report를 출력한 결과!
    - precision : 실제 0인 값들 중, 0으로 정확하게 예측한 비율이 94%
    - precision : 실제 1인 값들 중, 1로 정확하게 예측한 비율이 93%
        - 즉, svm을 활용해 높은 정확도로 분류를 수행했다는 것을 의미

---

## SVM 정리

## SVM(서포트 벡터 머신)

- 선형, 비선형의 분류 및 회귀, 이상치 탐색에 사용
- 중간 크기의 데이터 셋, 복잡한 분류 문제에 좋음
- 데이터를 선형으로 분리하는 최적의 선형 결정 경계를 찾는 알고리즘
    - 이를 통해, 분류되지 않은 새로운 점이 나타나면 경계의 어느 쪽에 속하는지 확인하여 분류 과제를 수행할 수 있게 도와줌
    
1. 속성이 2개일 때

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-21.png)

- 속성이 단순하게 2개만 존재할 때, 이를 분류한 결정경계는 직선으로 나타날 것

1. 속성이 3개일 때

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-22.png)

- 이런 경우는, 결정경계는 직선이 아닌 평면이 됨 → 이를 초평면이라고 함

가장 좋은 결정경계는 무엇인가?

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-23.png)

- F의 그림이 가장 좋은 결정 경계라고 할 수 있음
    - 결정 경계는 데이터 군으로부터 최대한 멀리 떨어지는게 좋기 때문에, F를 보면 결정경계로 부터 두 집단의 거리가 나머지 그림보다 멀다는 것을 알 수 있음
    - 서포트 벡터 : 결정경계와 가까이 있는 데이터 포인터를 의미 → 이 포인터는 경계를 정의하는 결정적인 역할을 함

### 마진이란?

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-24.png)

- 결정 경계와 서포트 벡터 사이의 거리를 의미
    - 가장 중간에 존재하는 직선인 결정경계와 점선과의 거리가 마진임
    - 최적의 결정경계 → 마진을 최대화 시킴(결정경계와 데이터 집단의 거리가 멀수록 분류를 잘한 것이기 때문)

## 하드 마진 VS 소프트 마진

- 이상치를 얼마나 허용하느냐에 따라 마진의 종류가 달라짐
- 하드 마진
    
    ![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-25.png)
    
    - 이상치를 허용X, 기준을 까다롭게 세워 서포트 벡터와 결졍 경계 사이의 거리가 매우 좁게 설정한 것이 하드 마진임
    - 개별적인 학습 D를 다 놓치지 않으려고 이상치를 허용하지 않음 → 이러면 과대적합 문제가 발생할 수 있음

- 소프트 마진
    
    ![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-26.png)
    
    - 이상치를 마진 안에 어느 정도 포함되도록 너그럽게 기준을 잡은 것이 소프트 마진임
    - 기준을 너그럽게 잡음 → 결정경계와 서포트 벡터 사이의 거리가 멀어짐
    - 단점은 마진이 커져 대충 학습해질 수 있기 때문에 underfitting 문제가 발생할 수 있음
    

## SVM의 파라미터

### 파라미터 C

- SVM 모델이 오류를 어느정도 허용할 것인지를 지정할 수 있음
- 디폴트 값 = 1
- C값이 크면? → 하드마진(오류 허용X)
- C값이 작으면? → 소프트마진(오류 허용)
- C의 최적값은 데이터마다 다르기 때문에, 여러가지 C값을 넣어 모델을 검증해봐야 함

### 커널(kernel)

- 선형, 비선형 데이터에 따라 커널을 지정하는 것이 다름

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-27.png)

- if 이런 데이터가 있다면 → 직선으로 데이터를 분류할 수 없음
    - svm 모델은 kernel=poly를 통해 이를 해결할 수 있음
    
    ![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-28.png)
    
    - 다항식(polynomial) 커널을 사용하면 점들이 3차원으로 표시되면서 초평면의 결정경계를 얻어 분류를 수행할 수 있게 됨

- kernel의 종류 : linear, poly, rbf, sigmoid, precomputed → 디폴트 값 = rbf
    - rbf가 무엇인가?
        - 가우시안 커널이라고도 부름
        - 2차원의 점을 무한한 차원의 점으로 변환시켜줌

### 파라미터 gamma

- 결정 경계를 얼마나 유연하게 그을 것인지를 정해주는 것
    - gamma 값이 너무 높이면?

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-29.png)

               -  학습데이터에 너무 의존하는 것을 파악할 수 있음 → 과대적합이 발생하게 됨

- gamma값을 너무 낮추면?

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-30.png)

- 결정 경계를 너무 대충 그려 다른 집단에 속하는 데이터도 결정경계 안에 포함시켜 분류하게 됨 → 언더 피팅 발생

- 적당한 gamma값은?

![Untitled](https://github.com/bjh0507/Study/blob/main/%EC%95%84%EB%A7%88%EC%A1%B4%EB%8D%B0%EC%9D%B4%ED%84%B0_EDA/5-31.png)

- 적절하게 결정 경계를 그어줌

[https://velog.io/@daydream/SVMSupport-Vector-Machine](https://velog.io/@daydream/SVMSupport-Vector-Machine)

svm 설명

[https://velog.io/@hyunicecream/GridSearchCV란-어떻게-사용할까](https://velog.io/@hyunicecream/GridSearchCV%EB%9E%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%82%AC%EC%9A%A9%ED%95%A0%EA%B9%8C)

gridsearchcv 설명
